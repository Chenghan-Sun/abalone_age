---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. It is the main r file including the related code for the project for the STA 206.

```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(car)
library(leaps)
```

### Part I: data loading and feature engineering
#### Exploratory data analysis
##### Summary statistics
We first read in the data and develope an exploratory data analysis.
```{r}
col_names <- c('sex','length','diameter','height','whole','shucked','viscera','shell','rings')
data <- read.table('../dataset/abalone.txt', header=FALSE, sep=',', col.names= col_names)
is.na(data$frame) #check missing value 
summary(data)
```

It is interested to see the height data has minimum of zero. Aparrently, this is not possible. It might be the reason of the wrong input, but these data points should be excluded.
```{r}
#d_data <- data[data$height == 0,] #get index of error data 
#index_d_data <- as.numeric(rownames(d_data))
#data <- data[-(index_d_data)]
data <- subset(data, data$height != 0)
summary(data)
```
2 observed error data were deleted.

##### Type of data
```{r}
sapply(data, class)
```
It's observed that only sex is qualitative variable, we'll consider indicator variable later.

Also the predictor `whole` should be the linear function of variables `shucked`, `viscera` and `shell`, which is `whole` = `shucked` + `viscera`+ `shell` + weight loss during weighing operations 
```{r}
data$weight.loss <- data$whole - data$shucked - data$viscera - data$shell
nrow(data[data$weight.loss < 0,]) # number of data with whole weight less than sum of other three weights 
data <- subset(data, data$weight.loss >= 0)
summary(data)
```
153 data were removed.

##### 
The ggpairs plot below has same functionality as matrix scatter plot, boxplots, and histogram plots
```{r fig.height=10, fig.width=12, message=FALSE, warning=FALSE}
ggpairs(data, aes(colour = sex, alpha = 0.6), title="Pairs plot for abalone dataset") + theme_grey(base_size = 8)
```
1. Determination of response variable: rings. 
2. High correlation -- multicoliearity. 
3. Distributions of F and M are similar w.r.t all predictors --> simplify indicator variable to I and N.
4. Distribution of response are left-skewed -- box-cox.

Pan plots of sex (need pie charts for all predictors?)
```{r}
sex_value <- c(1307, 1342, 1528)
sex <- c("F", "I", "M")
piepercent <- round(100*sex_value/sum(sex_value), 1)
pieplot <- pie(sex_value, labels = piepercent, main="Abalone sex pie chart", col = rainbow(length(sex_value)))
legend("topright", sex, cex = 0.8, fill = rainbow(length(sex_value)))
```
Create a new variable N = M + F, change to binary indicator variable 
```{r}
data['sex'] <- ifelse(data$sex == 'I', 'I', 'N')
data$sex <- as.factor(data$sex)
summary(data)
```
# Part II
Data splitting (training~70%, validation~30%)
```{r}
set.seed(40)  #set seed for random number generator
n_data <- nrow(data)
indexes <- sample(1:n_data, size = 0.3*n_data)
data_validation <- data[indexes,]
data_train <- data[-indexes,]
```
Examine the similarity of training set and validation set
```{r}
par(mfrow=c(3,3))
boxplot(data_train$length, data_validation$length, col = 'orange', main = 'length', names=c('train', 'validation'))
boxplot(data_train$diameter, data_validation$diameter, col = 'orange', main = 'diameter', names=c('train', 'validation'))
boxplot(data_train$height, data_validation$height, col = 'orange', main = 'height', names=c('train', 'validation'))
boxplot(data_train$whole, data_validation$whole, col = 'orange', main = 'whole weight', names=c('train', 'validation'))
boxplot(data_train$shucked, data_validation$shucked, col = 'orange', main = 'shucked weight', names=c('train', 'validation'))
boxplot(data_train$viscera, data_validation$viscera, col = 'orange', main = 'viscera weight', names=c('train', 'validation'))
boxplot(data_train$shell, data_validation$shell, col = 'orange', main = 'shell weight', names=c('train', 'validation'))
boxplot(data_train$rings, data_validation$rings, col = 'orange', main = 'rings', names=c('train', 'validation'))
```
The data distribution looks similar. 

The first model: Additive Multiple Linear Regression Model: (consider subsets selection from the pool of all first-order effects of the 8 predictors)
```{r}
Model1 <- lm(rings~sex+length+diameter+height+whole+shucked+viscera+shell, data = data_train)
summary(Model1)
layout(matrix(c(1,2),1,2))
plot(Model1, which =c(1,2), col='blue')
```
- No equal variance, Q-Q plot shows
- The factor level `I` is the reference level of `sex` predictor.
- From the additive model, predictor `length` is not significant, since `length` and `diameter` are highly correlated. All weight predictors are significant.
- MSE
```{r}
anova(Model1)['Residuals', 3]
```
Box-Cox Transformations:
```{r}
boxcox(Model1)
```
lambda ~ 0 indicates logrithm transformation of rings 
```{r fig.height=2, fig.width=5, message=FALSE, warning=FALSE} 
model2 <- lm(log(rings)~sex+length+diameter+height+whole+shucked+viscera+shell, data=data_train)
summary(model2)
anova(model2)['Residuals', 3]
layout(matrix(c(1,2,3),1,3))
plot(model2, which =c(1,2), col='blue')
boxcox(model2)
```
- Residuals vs. fitted value plot still shows nonlinearity, and QQ plot shows right-skewed
We can see R2 increase to 0.6035 and MSE is much smaller.

#### Residual plots with the interaction terms

```{r fig.height=8, fig.width=5, message=FALSE, warning=FALSE} 
res_m2 <- resid(model2)
par(mfrow=c(7,3))
plot(data_train$length*data_train$diameter,res_m2,xlab="length*diameter")
plot(data_train$length*data_train$height,res_m2,xlab="length*height")
plot(data_train$length*data_train$whole,res_m2,xlab="length*whole")
plot(data_train$length*data_train$shucked,res_m2,xlab="length*shucked")
plot(data_train$length*data_train$viscera,res_m2,xlab="length*viscera")
plot(data_train$length*data_train$shell,res_m2,xlab="length*shell")

plot(data_train$diameter*data_train$height,res_m2,xlab="diameter*height")
plot(data_train$diameter*data_train$whole,res_m2,xlab="diameter*whole")
plot(data_train$diameter*data_train$shucked,res_m2,xlab="diameter*schuked")
plot(data_train$diameter*data_train$viscera,res_m2,xlab="diameter*viscera")
plot(data_train$diameter*data_train$shell,res_m2,xlab="diameter*shell")

plot(data_train$height*data_train$whole,res_m2,xlab="height*whole")
plot(data_train$height*data_train$shucked,res_m2,xlab="height*shucked")
plot(data_train$height*data_train$viscera,res_m2,xlab="height*viscera")
plot(data_train$height*data_train$shell,res_m2,xlab="height*shell")

plot(data_train$whole*data_train$shucked,res_m2,xlab="whole*shucked")
plot(data_train$whole*data_train$viscera,res_m2,xlab="whole*viscera")
plot(data_train$whole*data_train$shell,res_m2,xlab="whole*shell")

plot(data_train$shucked*data_train$viscera,res_m2,xlab="shucked*viscera")
plot(data_train$shucked*data_train$shell,res_m2,xlab="shucked*shell")

plot(data_train$viscera*data_train$shell,res_m2,xlab="viscera*shell")
```

Pair plot for Model2
```{r fig.height=10, fig.width=12, message=FALSE, warning=FALSE}
# make another dataset with log(rings) for pair plot
data_log <- data_train
data_log[,9] <- log(data_log[,9])
ggpairs(data_log, aes(colour = sex, alpha = 0.6), title="Pairs plot for data in training set after log-transformation") + theme_grey(base_size = 8)
```
No obvious nonlinearity observed.

Multicolinearity 
```{r}
vif(Model2)
```
`whole` has largest vif.

added variable plot:
```{r}
Model2_a <- lm(log(rings)~sex+length+diameter+height+shucked+viscera+shell, data=data_train)
Model2_wo_whole <- residuals(lm(log(rings)~sex+length+diameter+height+shucked+viscera+shell, data=data_train))
fit_whole <- residuals(lm(whole~sex+length+diameter+height+shucked+viscera+shell, data = data_train))
plot(Model2_wo_whole~fit_whole, col='blue', main='added variable plot of whole weight')
abline(lm(Model2_wo_whole~fit_whole), col='red')
```
Added-variable plot for whole implies that whole is of little additional help in explaining rings when other predictors are already in the model.

vif for Model2_a
```{r}
vif(Model2_a)
```
Three terms `length`, `diameter`, and `viscera` still have high VIF. Exclude `diameter` since it has largest VIF.
```{r}
Model2_b <- lm(log(rings)~sex+length+height+shucked+viscera+shell, data = data_train)
Model2_wo_diameter <- residuals(lm(log(rings)~sex+length+height+shucked+viscera+shell, data = data_train))
fit_diameter <- residuals(lm(diameter~sex+length+height+shucked+viscera+shell, data = data_train))
plot(Model2_wo_diameter~fit_diameter, col='blue', main='added variable plot of diameter')
abline(lm(Model2_wo_diameter~fit_diameter), col='red')
```
vif for Model1_b
```{r}
vif(Model2_b)
```
VIF for each predictor looks better. But note that we couldn't drop any predictors only based on multicoliearity analysis. 

Part III
Model selection: 
Exhaustive search
```{r}
sub_set <- regsubsets(log(rings)~sex+length+diameter+height+whole+shucked+viscera+shell,data = data_train, nbest = 1, nvmax = 8, method="exhaustive", really.big=T)
sum_sub <- summary(sub_set)
p = as.integer(rownames(sum_sub$which)) + 1 #number of coefficients in each model: p
ssto = sum((log(data_train[,9]) - mean(log(data_train[,9])))^2)
n=nrow(data_train)
sse <- sum_sub$rss
AIC <- n*log(sse/n) + 2*p
BIC <- n*log(sse/n) + log(n)*p
res_sub = cbind(sum_sub$which,sse,sum_sub$rsq,sum_sub$adjr2,sum_sub$cp,BIC,AIC)
res_sub
```

Using stepwise procedure:
```{r}
Model0 <- lm(log(rings)~1, data = data_train)
# use BIC as criteria since n >> 8
n <- length(resid(Model2))
step_forward_BIC <- stepAIC(Model0, scope=list(upper=Model2), direction="both", k=log(n))
step_forward_BIC$anova
```
Both AIC and BIC give except `length`.

Check Model Diag 








